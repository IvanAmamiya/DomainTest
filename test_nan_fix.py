#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•Self-Attentionæ¨¡å—ä¿®å¤
éªŒè¯NaNé—®é¢˜æ˜¯å¦å·²è§£å†³
"""

import torch
import torch.nn as nn
from models import SelfAttentionResNet34
import warnings
warnings.filterwarnings('ignore')

def test_self_attention_nan():
    """æµ‹è¯•Self-Attentionæ¨¡å—æ˜¯å¦è¿˜ä¼šäº§ç”ŸNaN"""
    print("ğŸ” æµ‹è¯•Self-Attention NaNä¿®å¤...")
    
    # åˆ›å»ºæ¨¡å‹
    model = SelfAttentionResNet34(num_classes=10, input_channels=3)
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 4
    test_input = torch.randn(batch_size, 3, 224, 224)
    
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"è¾“å…¥èŒƒå›´: [{test_input.min().item():.3f}, {test_input.max().item():.3f}]")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    with torch.no_grad():
        try:
            output = model(test_input)
            
            print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")
            
            # æ£€æŸ¥NaN
            has_nan = torch.isnan(output).any()
            has_inf = torch.isinf(output).any()
            
            if has_nan:
                print("âŒ è¾“å‡ºåŒ…å«NaN!")
                return False
            elif has_inf:
                print("âŒ è¾“å‡ºåŒ…å«Inf!")
                return False
            else:
                print("âœ… è¾“å‡ºæ­£å¸¸ï¼Œæ— NaNæˆ–Inf")
                return True
                
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å‡ºé”™: {e}")
            return False

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµæ˜¯å¦æ­£å¸¸"""
    print("\nğŸ” æµ‹è¯•æ¢¯åº¦æµ...")
    
    model = SelfAttentionResNet34(num_classes=10, input_channels=3)
    model.train()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(2, 3, 224, 224, requires_grad=True)
    y = torch.randint(0, 10, (2,))
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    try:
        # å‰å‘ä¼ æ’­
        output = model(x)
        loss = criterion(output, y)
        
        print(f"Loss: {loss.item():.6f}")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_norm = 0
        param_count = 0
        nan_grad_count = 0
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                grad_norm += param_norm.item() ** 2
                param_count += 1
                
                if torch.isnan(param.grad).any():
                    print(f"âŒ {name} çš„æ¢¯åº¦åŒ…å«NaN")
                    nan_grad_count += 1
        
        grad_norm = grad_norm ** 0.5
        
        print(f"å‚æ•°æ•°é‡: {param_count}")
        print(f"æ€»æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        print(f"NaNæ¢¯åº¦æ•°é‡: {nan_grad_count}")
        
        if nan_grad_count == 0 and grad_norm < 100:  # åˆç†çš„æ¢¯åº¦èŒƒæ•°
            print("âœ… æ¢¯åº¦æµæ­£å¸¸")
            return True
        else:
            print("âŒ æ¢¯åº¦æµå¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµ‹è¯•å‡ºé”™: {e}")
        return False

def test_attention_weights():
    """æµ‹è¯•Attentionæƒé‡çš„æ•°å€¼ç‰¹æ€§"""
    print("\nğŸ” æµ‹è¯•Attentionæƒé‡...")
    
    from models import SelfAttentionModule
    
    # æµ‹è¯•ä¸åŒé€šé“æ•°
    for channels in [64, 128, 256, 512]:
        print(f"\næµ‹è¯• {channels} é€šé“:")
        
        attention = SelfAttentionModule(channels)
        attention.eval()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        h, w = 14, 14  # å…¸å‹çš„ç‰¹å¾å›¾å¤§å°
        x = torch.randn(2, channels, h, w)
        
        with torch.no_grad():
            try:
                output = attention(x)
                
                # æ£€æŸ¥è¾“å‡º
                has_nan = torch.isnan(output).any()
                has_inf = torch.isinf(output).any()
                
                print(f"  è¾“å…¥èŒƒå›´: [{x.min().item():.3f}, {x.max().item():.3f}]")
                print(f"  è¾“å‡ºèŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")
                print(f"  Gammaå€¼: {attention.gamma.item():.6f}")
                
                if has_nan:
                    print(f"  âŒ {channels}é€šé“ - è¾“å‡ºåŒ…å«NaN")
                elif has_inf:
                    print(f"  âŒ {channels}é€šé“ - è¾“å‡ºåŒ…å«Inf")
                else:
                    print(f"  âœ… {channels}é€šé“ - æ­£å¸¸")
                    
            except Exception as e:
                print(f"  âŒ {channels}é€šé“ - å‡ºé”™: {e}")

if __name__ == "__main__":
    print("=" * 50)
    print("Self-Attention NaNä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    test1 = test_self_attention_nan()
    test2 = test_gradient_flow() 
    test_attention_weights()
    
    print("\n" + "=" * 50)
    if test1 and test2:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼NaNé—®é¢˜å·²ä¿®å¤")
    else:
        print("âš ï¸  ä»å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    print("=" * 50)
