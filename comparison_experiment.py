#!/usr/bin/env python3
"""
ResNet34 vs Self-Attention ResNet34 å¯¹æ¯”å®éªŒ
ç³»ç»Ÿæ€§æ¯”è¾ƒä¸¤ç§æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½
"""

import os
import json
import time
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
import copy

from config_manager import load_config, setup_experiment
from data_loader import create_dataloader
from models import create_resnet_model, create_self_attention_resnet18, get_model_info
from trainer import DomainGeneralizationTrainer
from results_logger import create_results_logger


class ComparisonExperiment:
    """å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, config_path='config.yaml'):
        self.config = load_config(config_path)
        self.results = {}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path(f"results/comparison_{self.timestamp}")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def run_single_model_experiment(self, model_type, dataset_name, test_env):
        """è¿è¡Œå•ä¸ªæ¨¡å‹çš„å®éªŒ"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œå®éªŒ: {model_type} on {dataset_name} (test_env: {test_env})")
        print(f"{'='*60}")
        
        # å‡†å¤‡é…ç½®
        config = copy.deepcopy(self.config)
        config['model']['type'] = model_type
        config['dataset']['name'] = dataset_name
        config['dataset']['test_env'] = test_env
        
        # Update results path in config
        config['output']['results_path'] = f"{self.results_dir}/{model_type}_{dataset_name}_env{test_env}"
        
        # åˆ›å»ºå®éªŒæ—¥å¿—
        logger = create_results_logger(
            config
        )
        
        try:
            # åŠ è½½æ•°æ®
            data_loader = create_dataloader(config)
            train_loaders, test_loaders = data_loader.get_dataloaders()
            dataset_info = data_loader.get_dataset_info()
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªè®­ç»ƒç¯å¢ƒä½œä¸ºè®­ç»ƒé›†ï¼Œç¬¬ä¸€ä¸ªæµ‹è¯•ç¯å¢ƒä½œä¸ºæµ‹è¯•é›†
            train_loader = train_loaders[0] if train_loaders else None
            test_loader = test_loaders[0] if test_loaders else None
            
            if train_loader is None or test_loader is None:
                raise ValueError("æ— æ³•è·å–è®­ç»ƒæˆ–æµ‹è¯•æ•°æ®åŠ è½½å™¨")
            
            input_shape = dataset_info['input_shape']
            num_classes = dataset_info['num_classes']
            
            # åˆ›å»ºéªŒè¯é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ï¼‰
            val_loader = train_loader  # ç®€åŒ–å¤„ç†ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥åˆ†å‰²æ•°æ®é›†
            
            # åˆ›å»ºæ¨¡å‹
            if model_type == 'resnet18':
                model = create_resnet_model(
                    num_classes=num_classes,
                    input_channels=input_shape[0],
                    pretrained=config['model']['pretrained'],
                    model_type='resnet18'
                ).to(self.device)
            elif model_type == 'selfattentionresnet18':
                model = create_self_attention_resnet18(
                    num_classes=num_classes,
                    input_channels=input_shape[0],
                    pretrained=config['model']['pretrained']
                ).to(self.device)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = get_model_info(model, model_type)
            model_info.update({
                'input_channels': input_shape[0],
                'input_size': input_shape[1] if len(input_shape) > 1 else None,
                'num_classes': num_classes
            })
            
            print(f"æ¨¡å‹ä¿¡æ¯: {model_info}")
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = DomainGeneralizationTrainer(
                model=model, 
                train_loaders=train_loaders, # Pass all train_loaders
                test_loaders=test_loaders,   # Pass all test_loaders
                config=config, 
                device=self.device
            )
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # è®­ç»ƒæ¨¡å‹
            # trainer.train() æ–¹æ³•æœŸæœ›çš„å‚æ•°æ˜¯ num_epochs
            # trainer.train() è¿”å› train_history, test_history, best_test_acc
            train_history, test_history, best_test_accuracy_from_train = trainer.train(
                num_epochs=config['training']['epochs']
            )
            
            # æµ‹è¯•æ¨¡å‹
            # trainer.evaluate() æ–¹æ³•æœŸæœ›çš„å‚æ•°æ˜¯ loaders å’Œå¯é€‰çš„ split_name
            # è¿”å›å€¼æ˜¯ avg_loss, avg_accuracy, env_results
            _, test_accuracy, _ = trainer.evaluate(loaders=test_loaders) # ä½¿ç”¨æ‰€æœ‰çš„ test_loaders
            
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            training_time = time.time() - start_time
            
            # ä¿å­˜æ¨¡å‹
            model_path = self.results_dir / f"best_{model_type}_{dataset_name}_env{test_env}.pth"
            # The best model is stored in trainer.model after training
            torch.save(trainer.model.state_dict(), model_path)
            
            # æ”¶é›†ç»“æœ
            result = {
                'model_type': model_type,
                'dataset': dataset_name,
                'test_env': test_env,
                'test_accuracy': test_accuracy,
                'training_time': training_time,
                'model_info': model_info,
                'train_history': train_history,
                'test_history': test_history,  # æ·»åŠ æµ‹è¯•å†å²æ•°æ®
                'model_path': str(model_path),
                'success': True
            }
            
            # logger.log_final_results(result) # Incorrect method name
            # Corrected method call: log_experiment expects different arguments.
            # We need to construct the arguments as expected by log_experiment.
            
            # Construct dataset_info for the logger
            dataset_info_for_logger = {
                'name': dataset_name,
                'input_shape': input_shape,
                'num_classes': num_classes,
                'num_environments': len(train_loaders) + len(test_loaders) # Approximate or get from data_loader if available
            }
            
            # Construct training_summary for the logger
            training_summary_for_logger = {
                'final_train_acc': train_history['accuracy'][-1] if train_history and train_history['accuracy'] else 0,
                'final_test_acc': test_accuracy, # This is the overall test accuracy from trainer.evaluate
                'best_test_acc': best_test_accuracy_from_train, # This is the best test accuracy during training epochs
                'final_train_loss': train_history['loss'][-1] if train_history and train_history['loss'] else 0,
                'final_test_loss': test_history['loss'][-1] if test_history and test_history['loss'] else 0, # Assuming test_history is populated by trainer
                'total_training_time': training_time,
                'avg_epoch_time': training_time / config['training']['epochs'] if config['training']['epochs'] > 0 else 0
            }

            logger.log_experiment(
                dataset_info=dataset_info_for_logger,
                model_info=model_info,
                training_summary=training_summary_for_logger,
                train_history=train_history,
                test_history=test_history # trainer.train now returns test_history as the second value
            )
            
            print(f"å®éªŒå®Œæˆ! æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            return result
            
        except Exception as e:
            print(f"å®éªŒå¤±è´¥: {str(e)}")
            result = {
                'model_type': model_type,
                'dataset': dataset_name,
                'test_env': test_env,
                'error': str(e),
                'success': False
            }
            return result
    
    def run_comparison_experiments(self, datasets=None, test_envs=None):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
        if datasets is None:
            datasets = ['ColoredMNIST', 'TerraIncognita']
        
        if test_envs is None:
            test_envs = {
                'ColoredMNIST': [0, 1, 2],
                'TerraIncognita': [0, 1, 2, 3]
            }
        
        model_types = ['selfattentionresnet18', 'resnet18']
        
        print(f"å¼€å§‹å¯¹æ¯”å®éªŒ:")
        print(f"æ¨¡å‹ç±»å‹: {model_types}")
        print(f"æ•°æ®é›†: {datasets}")
        print(f"æµ‹è¯•ç¯å¢ƒ: {test_envs}")
        
        all_results = []
        
        for dataset in datasets:
            envs = test_envs.get(dataset, [0])
            for test_env in envs:
                for model_type in model_types:
                    result = self.run_single_model_experiment(model_type, dataset, test_env)
                    all_results.append(result)
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        results_file = self.results_dir / "comparison_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        # Pass the raw all_results to generate_analysis_report for epoch-level data access
        self.generate_analysis_report(all_results) 
        
        return all_results
    
    def generate_analysis_report(self, all_results_raw): # Renamed parameter to reflect it's raw data
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        print(f"{'='*80}")
        
        # è¿‡æ»¤æˆåŠŸçš„å®éªŒ
        successful_results = [r for r in all_results_raw if r.get('success', False)]
        
        if not successful_results:
            print("æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
            return
        
        # åˆ›å»ºDataFrame for summary statistics (as before)
        df_data = []
        for result in successful_results:
            df_data.append({
                'Model': result['model_type'],
                'Dataset': result['dataset'],
                'Test_Env': result['test_env'],
                'Test_Accuracy': result['test_accuracy'],
                'Final_Train_Accuracy': result['train_history']['accuracy'][-1] if result.get('train_history') and result['train_history'].get('accuracy') else None,
                'Training_Time': result['training_time'],
                'Total_Parameters': result['model_info']['total_parameters'] if result.get('model_info') else None,
                'Architecture': result['model_info']['architecture'] if result.get('model_info') else None
            })
        
        df = pd.DataFrame(df_data)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        csv_file = self.results_dir / "detailed_comparison.csv"
        df.to_csv(csv_file, index=False)
        
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        summary = self.generate_summary_statistics(df)
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.results_dir / "comparison_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨, pass both the summary DataFrame and the raw results for epoch data
        self.generate_comparison_plots(df, all_results_raw) # Pass raw results here
        
        # æ‰“å°æ‘˜è¦
        self.print_summary(summary)
    
    def generate_summary_statistics(self, df):
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        summary = {}
        
        for model in df['Model'].unique():
            model_df = df[df['Model'] == model]
            
            summary[model] = {
                'avg_test_accuracy': float(model_df['Test_Accuracy'].mean()),
                'std_test_accuracy': float(model_df['Test_Accuracy'].std()),
                'avg_training_time': float(model_df['Training_Time'].mean()),
                'total_parameters': int(model_df['Total_Parameters'].iloc[0]),
                'architecture': model_df['Architecture'].iloc[0],
                'best_test_accuracy': float(model_df['Test_Accuracy'].max()),
                'worst_test_accuracy': float(model_df['Test_Accuracy'].min()),
                'avg_train_accuracy': float(model_df['Final_Train_Accuracy'].mean()) if 'Final_Train_Accuracy' in model_df and not model_df['Final_Train_Accuracy'].isnull().all() else None,
                'std_train_accuracy': float(model_df['Final_Train_Accuracy'].std()) if 'Final_Train_Accuracy' in model_df and not model_df['Final_Train_Accuracy'].isnull().all() else None,
                'best_train_accuracy': float(model_df['Final_Train_Accuracy'].max()) if 'Final_Train_Accuracy' in model_df and not model_df['Final_Train_Accuracy'].isnull().all() else None,
                'worst_train_accuracy': float(model_df['Final_Train_Accuracy'].min()) if 'Final_Train_Accuracy' in model_df and not model_df['Final_Train_Accuracy'].isnull().all() else None,
                'experiments_count': len(model_df)
            }
        
        # æ¯”è¾ƒåˆ†æ
        if len(summary) >= 2:
            models = list(summary.keys())
            model1, model2 = models[0], models[1]
            
            summary['comparison'] = {
                'accuracy_difference': summary[model2]['avg_test_accuracy'] - summary[model1]['avg_test_accuracy'] if summary[model1]['avg_test_accuracy'] is not None and summary[model2]['avg_test_accuracy'] is not None else None,
                'time_difference': summary[model2]['avg_training_time'] - summary[model1]['avg_training_time'],
                'parameter_difference': summary[model2]['total_parameters'] - summary[model1]['total_parameters'],
                'better_model_accuracy': model2 if summary[model1]['avg_test_accuracy'] is not None and summary[model2]['avg_test_accuracy'] is not None and summary[model2]['avg_test_accuracy'] > summary[model1]['avg_test_accuracy'] else model1 if summary[model1]['avg_test_accuracy'] is not None and summary[model2]['avg_test_accuracy'] is not None else "N/A",
                'faster_model': model1 if summary[model1]['avg_training_time'] < summary[model2]['avg_training_time'] else model2
            }
        
        return summary
    
    def generate_comparison_plots(self, df, all_results_raw):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        plt.style.use('default')
        plt.rcParams.update({'font.size': 10})

        # ä»åŸå§‹ç»“æœä¸­æå–æ¯ä¸ªepochçš„è®­ç»ƒå’Œæµ‹è¯•å†å²
        epoch_data = []
        for res in all_results_raw:
            if res.get('success', False) and 'train_history' in res and 'test_history' in res:
                model_type = res['model_type']
                test_env = res['test_env']
                train_losses = res['train_history'].get('loss', [])
                train_accs = res['train_history'].get('accuracy', [])
                test_losses = res.get('test_history', {}).get('loss', [])
                test_accs = res.get('test_history', {}).get('accuracy', [])
                max_epochs = max(len(train_losses), len(train_accs), len(test_losses), len(test_accs))
                
                for epoch in range(max_epochs):
                    epoch_data.append({
                        'Model': model_type,
                        'Test_Env': test_env,
                        'Epoch': epoch + 1,
                        'Train_Loss': train_losses[epoch] if epoch < len(train_losses) else None,
                        'Train_Accuracy': train_accs[epoch] if epoch < len(train_accs) else None,
                        'Test_Loss': test_losses[epoch] if epoch < len(test_losses) else None,
                        'Test_Accuracy_Epoch': test_accs[epoch] if epoch < len(test_accs) else None
                    })
        
        epoch_df = pd.DataFrame(epoch_data)

        # è®¡ç®—åŸŸæ³›åŒ–æ€§èƒ½æŒ‡æ ‡
        domain_generalization_metrics = self.calculate_domain_generalization_metrics(df)

        fig, axes = plt.subplots(2, 3, figsize=(24, 16))  # æ”¹ä¸º2x3å¸ƒå±€ä»¥å®¹çº³æ›´å¤šå›¾è¡¨
        
        # 1. æ¨¡å‹å‡†ç¡®ç‡ vs æµ‹è¯•ç¯å¢ƒ (æ ¸å¿ƒåŸŸæ³›åŒ–èƒ½åŠ›æŒ‡æ ‡)
        ax1 = axes[0, 0]
        models = sorted(df['Model'].unique())
        dataset_name_title = df['Dataset'].unique()[0] if len(df['Dataset'].unique()) == 1 else "Multiple Datasets"
        active_test_envs = sorted(df['Test_Env'].unique())
        x = np.arange(len(active_test_envs))
        num_models = len(models)
        if num_models > 0:
            total_width_for_group = 0.8
            bar_width = total_width_for_group / num_models
        else:
            bar_width = 0.8

        for i, model in enumerate(models):
            accuracies = []
            for env_val in active_test_envs:
                acc = df[(df['Model'] == model) & (df['Test_Env'] == env_val)]['Test_Accuracy'].mean()
                accuracies.append(acc if not pd.isna(acc) else 0)
            offset = (i - (num_models - 1) / 2) * bar_width
            bars = ax1.bar(x + offset, accuracies, bar_width, label=model, alpha=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, acc in zip(bars, accuracies):
                ax1.annotate(f'{acc:.3f}',
                            xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                            xytext=(0, 3), textcoords="offset points",
                            ha='center', va='bottom', fontsize=8)
        
        ax1.set_xlabel('æµ‹è¯•ç¯å¢ƒ (Test Environment)')
        ax1.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡ (Test Accuracy)')
        ax1.set_title(f'åŸŸæ³›åŒ–èƒ½åŠ›å¯¹æ¯” - è·¨ç¯å¢ƒæ€§èƒ½ ({dataset_name_title})')
        ax1.set_xticks(x)
        ax1.set_xticklabels([f"ç¯å¢ƒ {env}" for env in active_test_envs])
        ax1.legend(title="æ¨¡å‹")
        ax1.grid(True, linestyle='--', alpha=0.7)
        ax1.set_ylim(0, 1.05)

        # 2. è®­ç»ƒvsæµ‹è¯•å‡†ç¡®ç‡æ›²çº¿ (è¿‡æ‹Ÿåˆæ£€æµ‹)
        ax2 = axes[0, 1]
        
        if not epoch_df.empty:
            for model_type in epoch_df['Model'].unique():
                model_epoch_df = epoch_df[epoch_df['Model'] == model_type]
                numeric_cols = ['Train_Accuracy', 'Test_Accuracy_Epoch']
                available_cols = [col for col in numeric_cols if col in model_epoch_df.columns]
                avg_epoch_df = model_epoch_df.groupby('Epoch')[available_cols].mean().reset_index()
                
                if 'Train_Accuracy' in avg_epoch_df.columns:
                    ax2.plot(avg_epoch_df['Epoch'], avg_epoch_df['Train_Accuracy'], 
                            marker='o', linestyle='-', linewidth=2, markersize=3,
                            label=f'{model_type} è®­ç»ƒå‡†ç¡®ç‡')
                if 'Test_Accuracy_Epoch' in avg_epoch_df.columns:
                    ax2.plot(avg_epoch_df['Epoch'], avg_epoch_df['Test_Accuracy_Epoch'], 
                            marker='x', linestyle='--', linewidth=2, alpha=0.8, markersize=4,
                            label=f'{model_type} æµ‹è¯•å‡†ç¡®ç‡')
        
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
        ax2.set_ylabel('å‡†ç¡®ç‡ (Accuracy)')
        ax2.set_title('è®­ç»ƒvsæµ‹è¯•å‡†ç¡®ç‡ - æ³›åŒ–æ€§èƒ½ç›‘æ§')
        ax2.grid(True, linestyle='--', alpha=0.7)
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc='best')

        # 3. åŸŸæ³›åŒ–æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        ax3 = axes[0, 2]
        self.plot_domain_generalization_radar(ax3, domain_generalization_metrics, models)

        # 4. è®­ç»ƒæŸå¤±æ›²çº¿
        ax4 = axes[1, 0]
        
        if not epoch_df.empty:
            for model_type in epoch_df['Model'].unique():
                model_epoch_df = epoch_df[epoch_df['Model'] == model_type]
                numeric_cols = ['Train_Loss', 'Test_Loss']
                available_cols = [col for col in numeric_cols if col in model_epoch_df.columns]
                avg_epoch_df = model_epoch_df.groupby('Epoch')[available_cols].mean().reset_index()
                
                if 'Train_Loss' in avg_epoch_df.columns:
                    ax4.plot(avg_epoch_df['Epoch'], avg_epoch_df['Train_Loss'], 
                            marker='s', linestyle='-', linewidth=2, markersize=3,
                            label=f'{model_type} è®­ç»ƒæŸå¤±')
                if 'Test_Loss' in avg_epoch_df.columns and not avg_epoch_df['Test_Loss'].isnull().all():
                    ax4.plot(avg_epoch_df['Epoch'], avg_epoch_df['Test_Loss'], 
                            marker='^', linestyle='--', linewidth=2, alpha=0.8, markersize=4,
                            label=f'{model_type} æµ‹è¯•æŸå¤±')
        
        ax4.set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
        ax4.set_ylabel('æŸå¤± (Loss)')
        ax4.set_title('è®­ç»ƒvsæµ‹è¯•æŸå¤± - æ”¶æ•›æ€§åˆ†æ')
        ax4.grid(True, linestyle='--', alpha=0.7)
        ax4.legend(loc='best')
        
        # è®¾ç½®æŸå¤±å›¾çš„yè½´èŒƒå›´
        if not epoch_df.empty and (('Train_Loss' in epoch_df.columns and epoch_df['Train_Loss'].notna().any()) or 
                                   ('Test_Loss' in epoch_df.columns and epoch_df['Test_Loss'].notna().any())):
            min_loss = min(epoch_df['Train_Loss'].min() if 'Train_Loss' in epoch_df and epoch_df['Train_Loss'].notna().any() else float('inf'),
                          epoch_df['Test_Loss'].min() if 'Test_Loss' in epoch_df and epoch_df['Test_Loss'].notna().any() else float('inf'))
            max_loss = max(epoch_df['Train_Loss'].max() if 'Train_Loss' in epoch_df and epoch_df['Train_Loss'].notna().any() else 0,
                          epoch_df['Test_Loss'].max() if 'Test_Loss' in epoch_df and epoch_df['Test_Loss'].notna().any() else 0)
            
            if pd.notna(min_loss) and pd.notna(max_loss) and max_loss > min_loss and min_loss != float('inf'):
                ax4.set_ylim(max(0, min_loss - 0.1 * (max_loss - min_loss)), max_loss + 0.1 * (max_loss - min_loss))
            elif pd.notna(max_loss) and max_loss > 0:
                ax4.set_ylim(0, max_loss * 1.1)
            else:
                ax4.set_ylim(0, 1.0)

        # 5. åŸŸé—´æ€§èƒ½å·®å¼‚åˆ†æ (Domain Gap Analysis)
        ax5 = axes[1, 1]
        self.plot_domain_gap_analysis(ax5, df, models)

        # 6. æ¨¡å‹ç¨³å®šæ€§åˆ†æ - å‡†ç¡®ç‡åˆ†å¸ƒç®±çº¿å›¾
        ax6 = axes[1, 2]
        data_for_box = []
        valid_models_for_box = []
        colors = plt.cm.Set2(np.linspace(0, 1, len(models)))
        for model in models:
            accuracies_model_all_envs = df[df['Model'] == model]['Test_Accuracy'].dropna().values
            if len(accuracies_model_all_envs) > 0:
                data_for_box.append(accuracies_model_all_envs)
                valid_models_for_box.append(model)
        
        if data_for_box:
            box_plot = ax6.boxplot(data_for_box, labels=valid_models_for_box, patch_artist=True, widths=0.5)
            for patch, color in zip(box_plot['boxes'], colors[:len(valid_models_for_box)]):
                patch.set_facecolor(color)
                patch.set_alpha(0.8)
            ax6.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡ (Test Accuracy)')
            ax6.set_title('æ¨¡å‹ç¨³å®šæ€§åˆ†æ - è·¨åŸŸæ€§èƒ½åˆ†å¸ƒ')
            ax6.grid(True, linestyle='--', alpha=0.7)
            ax6.set_ylim(0, 1.05)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            for i, (model, data) in enumerate(zip(valid_models_for_box, data_for_box)):
                std = np.std(data)
                ax6.text(i+1, max(data) + 0.02, f'std: {std:.3f}', 
                        ha='center', va='bottom', fontsize=8)
        else:
            ax6.text(0.5, 0.5, "æ— æ•°æ®æ˜¾ç¤º", horizontalalignment='center', verticalalignment='center', transform=ax6.transAxes)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        fig.suptitle(f'åŸŸæ³›åŒ–å¯¹æ¯”å®éªŒ - å…¨é¢æ€§èƒ½åˆ†æ ({dataset_name_title})', fontsize=16, y=0.98)
        
        plot_file = self.results_dir / "comparison_plots.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"åŸŸæ³›åŒ–å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {plot_file}")
        
        # æ‰“å°åŸŸæ³›åŒ–æ€§èƒ½æ‘˜è¦
        self.print_domain_generalization_summary(domain_generalization_metrics)

    def calculate_domain_generalization_metrics(self, df):
        """è®¡ç®—åŸŸæ³›åŒ–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        for model in df['Model'].unique():
            model_df = df[df['Model'] == model]
            accuracies = model_df['Test_Accuracy'].values
            
            # åŸºæœ¬ç»Ÿè®¡
            mean_acc = np.mean(accuracies)
            std_acc = np.std(accuracies)
            min_acc = np.min(accuracies)
            max_acc = np.max(accuracies)
            
            # åŸŸæ³›åŒ–ç‰¹å®šæŒ‡æ ‡
            domain_gap = max_acc - min_acc  # æœ€å¤§åŸŸé—´æ€§èƒ½å·®å¼‚
            stability_score = 1 - (std_acc / mean_acc) if mean_acc > 0 else 0  # ç¨³å®šæ€§å¾—åˆ†
            worst_case_performance = min_acc  # æœ€å·®æƒ…å†µæ€§èƒ½
            consistency_score = 1 - (domain_gap / mean_acc) if mean_acc > 0 else 0  # ä¸€è‡´æ€§å¾—åˆ†
            
            metrics[model] = {
                'mean_accuracy': mean_acc,
                'std_accuracy': std_acc,
                'min_accuracy': min_acc,
                'max_accuracy': max_acc,
                'domain_gap': domain_gap,
                'stability_score': stability_score,
                'worst_case_performance': worst_case_performance,
                'consistency_score': consistency_score,
                'robust_score': (stability_score + consistency_score + worst_case_performance) / 3  # ç»¼åˆé²æ£’æ€§å¾—åˆ†
            }
        
        return metrics

    def plot_domain_generalization_radar(self, ax, metrics, models):
        """ç»˜åˆ¶åŸŸæ³›åŒ–æ€§èƒ½é›·è¾¾å›¾"""
        # å®šä¹‰é›·è¾¾å›¾çš„ç»´åº¦
        categories = ['å¹³å‡å‡†ç¡®ç‡', 'æœ€å·®æ€§èƒ½', 'ç¨³å®šæ€§', 'ä¸€è‡´æ€§', 'ç»¼åˆé²æ£’æ€§']
        
        # è®¡ç®—è§’åº¦
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # å®Œæˆåœ†ç¯
        
        ax = plt.subplot(2, 3, 3, projection='polar')
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
        
        for i, model in enumerate(models):
            if model in metrics:
                # å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°0-1èŒƒå›´
                values = [
                    metrics[model]['mean_accuracy'],
                    metrics[model]['worst_case_performance'],
                    metrics[model]['stability_score'],
                    metrics[model]['consistency_score'],
                    metrics[model]['robust_score']
                ]
                values += values[:1]  # å®Œæˆåœ†ç¯
                
                ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i % len(colors)])
                ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1)
        ax.set_title('åŸŸæ³›åŒ–æ€§èƒ½é›·è¾¾å›¾', y=1.08)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)

    def plot_domain_gap_analysis(self, ax, df, models):
        """ç»˜åˆ¶åŸŸé—´æ€§èƒ½å·®å¼‚åˆ†æ"""
        domain_gaps = []
        model_names = []
        
        for model in models:
            model_df = df[df['Model'] == model]
            if len(model_df) > 1:
                max_acc = model_df['Test_Accuracy'].max()
                min_acc = model_df['Test_Accuracy'].min()
                gap = max_acc - min_acc
                domain_gaps.append(gap)
                model_names.append(model)
        
        if domain_gaps:
            colors = ['#FF6B6B' if gap > 0.05 else '#4ECDC4' for gap in domain_gaps]
            bars = ax.bar(model_names, domain_gaps, color=colors, alpha=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, gap in zip(bars, domain_gaps):
                ax.annotate(f'{gap:.3f}',
                           xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                           xytext=(0, 3), textcoords="offset points",
                           ha='center', va='bottom', fontsize=9)
            
            ax.set_ylabel('æ€§èƒ½å·®å¼‚ (Max - Min Accuracy)')
            ax.set_title('åŸŸé—´æ€§èƒ½å·®å¼‚åˆ†æ\n(è¶Šå°è¡¨ç¤ºæ³›åŒ–èƒ½åŠ›è¶Šå¥½)')
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ å‚è€ƒçº¿
            ax.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='5%å·®å¼‚çº¿')
            ax.legend()
        else:
            ax.text(0.5, 0.5, "éœ€è¦å¤šä¸ªæµ‹è¯•ç¯å¢ƒæ•°æ®", 
                   horizontalalignment='center', verticalalignment='center', 
                   transform=ax.transAxes)

    def print_domain_generalization_summary(self, metrics):
        """æ‰“å°åŸŸæ³›åŒ–æ€§èƒ½æ‘˜è¦"""
        print(f"\n{'='*80}")
        print("åŸŸæ³›åŒ–æ€§èƒ½åˆ†æ")
        print(f"{'='*80}")
        
        for model, stats in metrics.items():
            print(f"\nğŸ”¸ {model}:")
            print(f"  å¹³å‡å‡†ç¡®ç‡: {stats['mean_accuracy']:.4f}")
            print(f"  æœ€å·®ç¯å¢ƒæ€§èƒ½: {stats['worst_case_performance']:.4f}")
            print(f"  åŸŸé—´æ€§èƒ½å·®å¼‚: {stats['domain_gap']:.4f}")
            print(f"  ç¨³å®šæ€§å¾—åˆ†: {stats['stability_score']:.4f}")
            print(f"  ä¸€è‡´æ€§å¾—åˆ†: {stats['consistency_score']:.4f}")
            print(f"  ç»¼åˆé²æ£’æ€§: {stats['robust_score']:.4f}")
            
        # æ‰¾å‡ºæœ€ä½³åŸŸæ³›åŒ–æ¨¡å‹
        if len(metrics) >= 2:
            best_model = max(metrics.keys(), key=lambda x: metrics[x]['robust_score'])
            print(f"\nğŸ† æœ€ä½³åŸŸæ³›åŒ–æ¨¡å‹: {best_model}")
            print(f"   ç»¼åˆé²æ£’æ€§å¾—åˆ†: {metrics[best_model]['robust_score']:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ ResNet34 vs Self-Attention ResNet34 å¯¹æ¯”å®éªŒ")
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    experiment = ComparisonExperiment()
    
    # ä¿®æ”¹é…ç½®ä»¥è¿›è¡Œepochä¸º300çš„å®éªŒ
    experiment.config['training']['epochs'] = 300 # æ¢å¤ä¸º300ä¸ªepochè¿›è¡Œå®Œæ•´è®­ç»ƒ
    experiment.config['model']['pretrained'] = False  # ä½¿ç”¨æœªé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”å®éªŒ
    
    # é…ç½®å®éªŒ
    datasets = ['ColoredMNIST']  # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®é›†
    test_envs = {
        'ColoredMNIST': [0, 1, 2]
    }
    
    # è¿è¡Œå®éªŒ
    results = experiment.run_comparison_experiments(datasets, test_envs)
    
    print(f"\nå®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {experiment.results_dir}")
    print(f"æ€»å…±å®Œæˆ {len([r for r in results if r.get('success', False)])} ä¸ªæˆåŠŸçš„å®éªŒ")


if __name__ == "__main__":
    main()
