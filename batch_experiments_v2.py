#!/usr/bin/env python3
"""
æ‰¹é‡å®éªŒè„šæœ¬
ç”¨äºè¿è¡Œå¤šç»„å¯¹æ¯”å®éªŒ
"""

import copy
import itertools
from main import run_single_experiment
from config_manager import load_config, setup_experiment
from results_logger import create_results_logger


def generate_experiment_configs(base_config):
    """ç”Ÿæˆæ‰¹é‡å®éªŒé…ç½®"""
    batch_config = base_config['batch_experiments']
    
    if not batch_config['enabled']:
        print("æ‰¹é‡å®éªŒæœªå¯ç”¨")
        return [base_config]
    
    # è·å–å®éªŒå‚æ•°ç»„åˆ
    datasets = batch_config['datasets']
    test_envs = batch_config['test_envs']
    learning_rates = batch_config['learning_rates']
    batch_sizes = batch_config['batch_sizes']
    
    # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    param_combinations = list(itertools.product(
        datasets, test_envs, learning_rates, batch_sizes
    ))
    
    configs = []
    for i, (dataset, test_env, lr, batch_size) in enumerate(param_combinations):
        config = copy.deepcopy(base_config)
        
        # æ›´æ–°å‚æ•°
        config['dataset']['name'] = dataset
        config['dataset']['test_env'] = test_env
        config['training']['learning_rate'] = lr
        config['training']['batch_size'] = batch_size
        
        # æ›´æ–°å®éªŒåç§°
        config['experiment']['name'] = f"batch_exp_{i+1:03d}_{dataset}_env{test_env}_lr{lr}_bs{batch_size}"
        
        configs.append(config)
    
    return configs


def run_batch_experiments(config_path='config.yaml'):
    """è¿è¡Œæ‰¹é‡å®éªŒ"""
    print("å¼€å§‹æ‰¹é‡å®éªŒ...")
    
    # åŠ è½½åŸºç¡€é…ç½®
    base_config = load_config(config_path)
    
    # ç”Ÿæˆå®éªŒé…ç½®
    experiment_configs = generate_experiment_configs(base_config)
    
    if len(experiment_configs) == 1:
        print("åªæœ‰ä¸€ä¸ªå®éªŒé…ç½®ï¼Œè¿è¡Œå•æ¬¡å®éªŒ")
        config, device = setup_experiment(config_path)
        run_single_experiment(config)
        return
    
    print(f"æ€»å…±å°†è¿è¡Œ {len(experiment_configs)} ä¸ªå®éªŒ")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = []
    successful_experiments = 0
    
    for i, config in enumerate(experiment_configs):
        print(f"\n{'='*80}")
        print(f"å®éªŒ {i+1}/{len(experiment_configs)}: {config['experiment']['name']}")
        print(f"{'='*80}")
        
        # è®¾ç½®å®éªŒç¯å¢ƒ
        try:
            # ç®€åŒ–è®¾ç½®ï¼Œåªæ›´æ–°å…³é”®é…ç½®
            from config_manager import setup_seed, setup_device, create_output_dirs, validate_config
            
            validate_config(config)
            setup_seed(config)
            device = setup_device(config)
            create_output_dirs(config)
            
            # è¿è¡Œå®éªŒ
            success, timestamp, best_acc = run_single_experiment(config)
            
            if success:
                successful_experiments += 1
                results.append({
                    'experiment_name': config['experiment']['name'],
                    'dataset': config['dataset']['name'],
                    'test_env': config['dataset']['test_env'],
                    'learning_rate': config['training']['learning_rate'],
                    'batch_size': config['training']['batch_size'],
                    'best_accuracy': best_acc,
                    'timestamp': timestamp,
                    'success': True
                })
                print(f"âœ“ å®éªŒæˆåŠŸ: {best_acc:.4f}")
            else:
                results.append({
                    'experiment_name': config['experiment']['name'],
                    'success': False
                })
                print("âœ— å®éªŒå¤±è´¥")
                
        except Exception as e:
            print(f"âœ— å®éªŒ {i+1} å‘ç”Ÿé”™è¯¯: {e}")
            results.append({
                'experiment_name': config['experiment']['name'],
                'success': False,
                'error': str(e)
            })
    
    # æ€»ç»“ç»“æœ
    print(f"\n{'='*80}")
    print("æ‰¹é‡å®éªŒå®Œæˆ!")
    print(f"{'='*80}")
    print(f"æ€»å®éªŒæ•°: {len(experiment_configs)}")
    print(f"æˆåŠŸå®éªŒ: {successful_experiments}")
    print(f"å¤±è´¥å®éªŒ: {len(experiment_configs) - successful_experiments}")
    
    # æ˜¾ç¤ºæˆåŠŸå®éªŒçš„ç»“æœ
    successful_results = [r for r in results if r.get('success', False)]
    if successful_results:
        print(f"\næˆåŠŸå®éªŒç»“æœ:")
        print("-" * 80)
        print(f"{'å®éªŒåç§°':<30} {'æ•°æ®é›†':<15} {'æµ‹è¯•ç¯å¢ƒ':<8} {'å­¦ä¹ ç‡':<10} {'æ‰¹å¤§å°':<8} {'æœ€ä½³å‡†ç¡®ç‡':<10}")
        print("-" * 80)
        
        for result in successful_results:
            print(f"{result['experiment_name']:<30} "
                  f"{result['dataset']:<15} "
                  f"{result['test_env']:<8} "
                  f"{result['learning_rate']:<10.6f} "
                  f"{result['batch_size']:<8} "
                  f"{result['best_accuracy']:<10.4f}")
        
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        best_result = max(successful_results, key=lambda x: x['best_accuracy'])
        print(f"\nğŸ† æœ€ä½³å®éªŒ: {best_result['experiment_name']}")
        print(f"   å‡†ç¡®ç‡: {best_result['best_accuracy']:.4f}")
        print(f"   å‚æ•°: æ•°æ®é›†={best_result['dataset']}, ç¯å¢ƒ={best_result['test_env']}, "
              f"å­¦ä¹ ç‡={best_result['learning_rate']}, æ‰¹å¤§å°={best_result['batch_size']}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    if successful_experiments > 1:
        print(f"\næ­£åœ¨ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
        try:
            results_logger = create_results_logger(base_config)
            results_logger.generate_comparison_plots()
            results_logger.print_summary_table()
        except Exception as e:
            print(f"ç”Ÿæˆå¯¹æ¯”å›¾è¡¨å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡å®éªŒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    run_batch_experiments(args.config)


if __name__ == '__main__':
    main()
